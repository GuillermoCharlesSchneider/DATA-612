{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "92d01b24-d6e3-4079-ac96-01af8b0cd61c",
      "cell_type": "markdown",
      "source": "## Research Discussion Assignment 3",
      "metadata": {}
    },
    {
      "id": "05f2b1c7-b18d-4a79-a0de-8efebf58411b",
      "cell_type": "markdown",
      "source": "In the talk by Evan Estola (2016) on \"When Recommendations Systems Go Bad\", he gives several examples of how recommendation systems can unknowingly reinforce human bias. The recommendation models that companies such as Google or Meetup, shape how we interact with the digital world. Often the systems don't just show the \"objectively best\" answer, but they introduce personalization into those results so that each user may get different results, often accidently reenforcing bias's. This can be great for user satisfaction, but Evan warned that recommendations extend past just search engines or shoe ads, but it effects your whole life: the news you see, whether you are filtered out of job postings, if you can get a loan, or get into universities. Mac users have been shown pricier hotels based on assumptions about spending habits, and Black-sounding names have been linked to more ads suggesting criminal records. Such systems often pick up on proxy variables like zip codes or names that strongly correlate with race or gender, even when that data isnâ€™t explicitly included. The consequence is that models, left unsupervised, can quietly reinforce societal inequalities.\n\nEthical responsibility lies with developers and platforms to ensure their models do not perpetuate harmful stereotypes or discriminatory patterns. For instance, Meetup recognized that its recommendation algorithm might infer women are less interested in tech events simply because of their underrepresentation, an observation it does not wish to reinforce. To counteract this, companies can use interpretable models that avoid problematic feature interactions or apply ensemble models that keep sensitive features separated during training. Developers must actively consider the social implications of their data and modeling choices. As seen in political and media contexts, recommendation engines can radicalize or bias users by feeding them increasingly extreme or hateful content, subtly shaping how they think, vote, and view the world. Ultimately, the challenge is not just building smart algorithms but ensuring they serve the public fairly and ethically.",
      "metadata": {}
    },
    {
      "id": "af35195a-c992-48b1-a8e1-2e6829cbb160",
      "cell_type": "markdown",
      "source": "Source: Evan Estola (2016): When Recommendations Systems Go Bad; MLconf SEA 2016",
      "metadata": {}
    },
    {
      "id": "75818028-287c-4e97-88a1-81fa6463ee5e",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}