{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "769cb6ed-ddc8-484f-8373-2dd91622d0bd",
      "cell_type": "code",
      "source": "#imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "5c7bd62d-51ee-4c34-834a-18b5bb28ef76",
      "cell_type": "markdown",
      "source": "# Project 3 | Matrix Factorization Methods ",
      "metadata": {}
    },
    {
      "id": "b5a4f632-d13f-48eb-a4b4-016bce4413f5",
      "cell_type": "markdown",
      "source": "## Jester Joke Data\n2.5 million anonymous ratings of jokes by users of the Jester Joke Recommender System (Ken Goldberg, AUTOLab, UC Berkeley).  Values from (-10.00 to +10.00) of 100 jokes collected between April 1999 - May 2003.  Data from 24,983 users who have rated 36 or more jokes, a matrix with dimensions 24983 X 101. ",
      "metadata": {}
    },
    {
      "id": "f05e9037-cd39-4da2-a768-b632286a6021",
      "cell_type": "code",
      "source": "#import joke data\njokes_df = pd.read_csv('jester-data-1.csv')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "a5c501a3-d6d6-4e5e-adc5-29d94f22249b",
      "cell_type": "markdown",
      "source": "### Sparseness -- Stochastic Gradient Descent\nThis dataset is relatively sparse. There are alot of jokes that users did not rate. High portion of missing values caused by sparseness in the user-item ratings matrix. Carelessly addressing only the relatively few known entries is highly prone to overfitting. Previously, I had used a dense subcluster to approximate user similarity. \n\nSVD requires that there are no missing values. This time i will use stochastic gradient descent (SGD) to prepare the data for matrix factorization. SGD loops through ratings in the training set. For each given training case, the system predicts rui and computes the associated prediction error portional to g in the opposite direction of the gradient.  Instead of using the full dataset to compute the gradient at each step, SGD uses only one random data point (or a small batch of data points) at each iteration. This makes the computation much faster.",
      "metadata": {}
    },
    {
      "id": "6aa557bd-e7a7-4e77-a556-7305f2a4c21a",
      "cell_type": "code",
      "source": "def sgd(X, y, learning_rate=0.1, epochs=1000, batch_size=1):\n    m = len(X)  \n    theta = np.random.randn(2, 1) \n    \n    X_bias = np.c_[np.ones((m, 1)), X]\n\n    cost_history = []  \n\n    for epoch in range(epochs):\n        indices = np.random.permutation(m)\n        X_shuffled = X_bias[indices]\n        y_shuffled = y[indices]\n\n        for i in range(0, m, batch_size):\n            X_batch = X_shuffled[i:i+batch_size]\n            y_batch = y_shuffled[i:i+batch_size]\n\n            gradients = 2 / batch_size * X_batch.T.dot(X_batch.dot(theta) - y_batch)\n            theta -= learning_rate * gradients\n\n        predictions = X_bias.dot(theta)\n        cost = np.mean((predictions - y) ** 2)\n        cost_history.append(cost)\n        \n        if epoch % 100 == 0:\n            print(f\"Epoch {epoch}, Cost: {cost}\")\n\n    return theta, cost_history",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "aa3ac80c-bfcc-45a6-910d-b38f1c496b62",
      "cell_type": "code",
      "source": "theta_final, cost_history = sgd(X, y, learning_rate=0.1, epochs=1000, batch_size=1)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7eceda10-d896-45a2-a088-7a3e63d10fe8",
      "cell_type": "markdown",
      "source": "### Visualizing the Cost Function:",
      "metadata": {}
    },
    {
      "id": "91d77498-cad0-4e19-b4e0-c265b39b3132",
      "cell_type": "code",
      "source": "plt.plot(cost_history)\nplt.xlabel('Epochs')\nplt.ylabel('Cost (MSE)')\nplt.title('Cost Function during Training')\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "dea68280-0180-4f2d-ad5a-7488b045795e",
      "cell_type": "markdown",
      "source": "### Plotting the Data and Regression Line",
      "metadata": {}
    },
    {
      "id": "e3acf20f-bb96-4a90-9cd5-f6c68aba9fbf",
      "cell_type": "code",
      "source": "plt.scatter(X, y, color='blue', label='Data points')\nplt.plot(X, np.c_[np.ones((X.shape[0], 1)), X].dot(theta_final), color='red', label='SGD fit line')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Linear Regression using Stochastic Gradient Descent')\nplt.legend()\nplt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c42691ff-91c2-4099-9c63-cfc2829488f7",
      "cell_type": "markdown",
      "source": "### Building a Complete UV-Decomposition Algorithm (Method: Gradient Descent)\n\t\t1. Preprocessing of the matrix M.\n\t\t\ti. Subtract the average rating of the user, and the average rating of the item\n\t\t\tii. We have to undo the normalization at the end. Add back the above\n\t\t2. Initializing U and V \n\t\t\ti. Simple starting is to give each element of U and V the same value. (probably 0 after normalization)\n\t\t3. Ordering the optimization of the elements of U and V \n\t\t\ti. Simplest is row-by-row, and visit them round robin fashion\n\t\t\tii. Note that just because we optimized an element once does not mean we cannot find a better value for that element after other elements have been adjusted. Thus, we need to visit elements repeatedly\n\t\t4. Ending the attempt at optimization.\n\t\t\ti. Ideally, at some point the RMSE becomes 0, and we know we cannot do better.\n\t\t\tii. In practice, since there are normally many more nonblank elements in M than there are elements in U and V together, we have no right to expect that we can reduce the RMSE to 0\nWe can track the amount of improvement in the RMSE obtained in one round of the optimization, and stop when that improvement falls below a threshold.",
      "metadata": {}
    },
    {
      "id": "79aa30f9-4945-4bf9-8113-56eb873d7876",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1628d004-b7f7-46fd-a48a-5fad3f6a9626",
      "cell_type": "markdown",
      "source": "## Matrix factorization models \nare superior to classic nearest-neighbor techniques for producing product recommendations, allowing the incorporation of additional information such as implicit feedback, temporal effects, and confidence levels.\t- Matrix factorization:\nmodels map both users and items to a joint latent factor space of dimensionality f, such that user-item interactions are modeled as inner products in that space.\nStorage reduction can be meaningful with large datasets, makes your matrixes smaller , without making your error too much higher:\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "id": "1e7bdc07-e8b8-4ab4-9756-6f5281dad375",
      "cell_type": "markdown",
      "source": "## Preprocessing: \nSVD can be thought of as a pre-processing step for feature engineering. You might easily start \nwith thousands or millions of items, and use SVD to create a much smaller set of “k” items \n(e.g. 20 or 70).",
      "metadata": {}
    },
    {
      "id": "0bf2c4c9-4484-4010-9ba1-17132b90b3ed",
      "cell_type": "markdown",
      "source": "## Reduce dimensionality:\nby applying SVD to transform a large user-item matrix into a smallerset of latent features, simplifying the data while preserving its essential patterns.",
      "metadata": {}
    },
    {
      "id": "60b42c10-8a69-4fa4-b07a-cb8939386472",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0962c81-03a7-497b-a334-079f7c23ec2f",
      "cell_type": "markdown",
      "source": "## Evaluate the effectiveness of the matrix factorization model:\nby assessing the quality of predictions using performance metrics such as RMSE, and comparing these results with those from\nbaseline models.  \n\nRoot-Mean-Square Error\n\t\t○ While we can pick among several measures of how close the product UV is to M, the typical choice is the root-mean-square error (RMSE), where we\n\t\t\ti. Sum, over all nonblank entries in M the square of the difference between that entry and the corresponding entry in the product UV .\n\t\t\tii. Take the mean (average) of these squares by dividing by the number of terms in the sum (i.e., the number of nonblank entries in M).\n\t\t\tiii. Take the square root of the mean\n\n\t- Local Minima:\n\t\t○ matrices U and V such that no allowable adjustment reduces the RMSE\n\t\t○ Global Minimum – Unfortunately, only one of these local minima will be the matrices U and V that produce the least possible RMSE\nThere is never a guarantee that our best local minimum will be the global minimum.",
      "metadata": {},
      "attachments": {}
    },
    {
      "id": "2338d231-50a2-469e-85d3-04f61078741b",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f564e2d1-1c4d-44e2-bcf0-26c270609aba",
      "cell_type": "markdown",
      "source": "## Overfitting: \nalthough the RMSE may be small on the given data, it doesn’t do well predicting future data\n\t\t○ UV-decomposition is that we arrive at one of the many local minima that conform well to the given data, but picks up values in the data that don’t reflect well the underlying process that gives rise to the data\n\t\t\n\t- Avoiding Overfitting:\n\t\t\t1. Avoid favoring the first components to be optimized by only moving the value of a component a fraction of the way, say half way, from its current value toward its optimized value.\n\t\t\t2. Stop revisiting elements of U and V well before the process has converged.\nTake several different UV decompositions, and when predicting a new entry in the matrix M, take the average of the results of using each decomposition.",
      "metadata": {},
      "attachments": {}
    },
    {
      "id": "ff2619ca-e2c2-4f31-8dc2-b32d7e9871a2",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6743191-cdfb-4d0a-b8f7-e3775fd85500",
      "cell_type": "markdown",
      "source": "## Understand the limitations and trade-offs of matrix factorization techniques, \nincluding the computational cost of SVD, the lack of explainability, and the challenges in handling sparse\ndatasets with many missing values.",
      "metadata": {}
    },
    {
      "id": "0df1c31a-feb4-45f9-b7c2-afa7c28f9ed7",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b6556d9b-acf5-4902-9621-559d4f0751df",
      "cell_type": "markdown",
      "source": "## Implement optimizations to improve the scalability and efficiency of matrix factorization mod\u0002els,\nincluding using subsets of data, parallelization, or other advanced techniques to manage memory and computation costs.",
      "metadata": {}
    },
    {
      "id": "b82e85f5-4f61-4dcf-8f49-75740999abd9",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}