{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "ad07a8d0-fcb1-40ff-a9fd-f534e61ac58e",
      "cell_type": "markdown",
      "source": "## Research Discussion Assignment 4",
      "metadata": {}
    },
    {
      "id": "3966a0b8-9553-43a2-9f6e-256984f53833",
      "cell_type": "markdown",
      "source": "In the \"Up Next: A Better Recommendation System\" article, Renee Diresta covers how Recommendation models can amplify conspiracy theories, gamified news, fake news, and misinform voters through further suggestions into those topics. The goal of these systems is to keep people clicking, by showing them more similar content, but the systems don't understand the content or differentiate between harmful content. Some companies have taken a hands-off approach on content moderation, citing free speech and user choice as the main reasons, but they are responsible as their presentation of choices has an impact on what people can choose. If possible, it would be good to allow content to exist on their platform, without algorithmically amplifying it.\n\nIn the past, companies have spontaneously cracked down on content related to suicide, pro-anorexia, payday lending, and bitcoin scams. This is usually in response to public outcry, and utilizes a simple keyword bans to minimize all talk on the subject. I think this can be a good solution to a sudden trending unexpected harmful topic, but it lacks nuance and broader applicability. This is a trend I've seen on social media for users to self censor words like \"kill\", \"suicide\", \"death\", \"rape\", or most other cursing on Tiktok, because of their fear of being \"shadow-banned\" by discussing these harmful topic keywords, even if their content is not extremeist in nature. Another example is Project Redirect, Google Jigsaw has working on a solution to users who are consistently searching Youtube for terrorist content. Once flagged, they work to create a recommendation system that does the opposite of showing more violent content, it shows content intended to deracialize them. \n\n\n\n\nRenee Diresta, Wired.com (2018): Up Next: A Better Recommendation System\n\n",
      "metadata": {},
      "attachments": {}
    },
    {
      "id": "6d528b54-e6c2-4ae1-ac76-d6d080770894",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}