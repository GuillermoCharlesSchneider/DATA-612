{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "ad07a8d0-fcb1-40ff-a9fd-f534e61ac58e",
      "cell_type": "markdown",
      "source": "## Research Discussion Assignment 4",
      "metadata": {}
    },
    {
      "id": "3966a0b8-9553-43a2-9f6e-256984f53833",
      "cell_type": "markdown",
      "source": "In the \"Up Next: A Better Recommendation System\" article, Renee Diresta covers how Recommendation models can amplify conspiracy theories, gamified news, fake news, and misinform voters through further suggestions into those topics. The goal of these systems is to keep people clicking, by showing them more similar content, but the systems don't understand the content or differentiate between harmful content. Some companies have taken a hands-off approach on content moderation, citing free speech and user choice as the main reasons, but they are responsible as their presentation of choices has an impact on what people can choose. \n\nIn the past, companies have spontaneously cracked down on content related to suicide, pro-anorexia, payday lending, and bitcoin scams. This is usually in response to public outcry, and utilizes a simple keyword bans to minimize all talk on the subject. I think this can be a good solution to a sudden trending unexpected harmful topic, but it lacks nuance and broader applicability. This is a trend I've seen on social media for users to self censor words like \"kill\", \"suicide\", \"death\", \"rape\", or most other cursing on Tiktok, because of their fear of being \"shadow-banned\" by discussing these harmful topic keywords, even if their content is not extremeist in nature. \n\nAnother example is Project Redirect, Google Jigsaw has working on a solution to users who are consistently searching Youtube for terrorist content. Once flagged, they work to create a recommendation system that does the opposite of showing more violent content, it shows content intended to deracialize them. Reading about this has reminded me of how we often talk about overfitting being a problem in recommendation models, where it's been trained so well on the test data, that it fails to predict data outside that. I think people who stuck in these online bubbles I think have also socially overfitted themselves. By only showing themselves (or being shown) content that fits one way, they are unable to predict views outside of it. \n\nRenee Diresta, Wired.com (2018): Up Next: A Better Recommendation System",
      "metadata": {},
      "attachments": {}
    }
  ]
}