{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "07f80106-e813-4eb2-a4df-835693dadd96",
      "cell_type": "markdown",
      "source": "### Research Discussion Assignment 2",
      "metadata": {}
    },
    {
      "id": "0673656a-2ef7-4a49-a44a-3d032741dd21",
      "cell_type": "markdown",
      "source": "This talk by Christopher Johnson focused on Music Recommendation models at Spotify, and how they started to experiment with Spark for this purpose. As a music streaming service they have a large catalog of 40 million songs, allows their many users to discover new music. They have three main recommendations purposes: Personalized recommendations based on what you've been listening to, \"radio stations\" for an artist or song, and showing similar artists to one's you like. The simplest method for recommendation include manually curating the music, but this only works for smaller catalogs. A step more complex, Pandora created the Music Genome Project to manually tag music attributes, and try to curate songs based on those attributes. Finally, Spotify is relying mostly on collaborative filtering now, to look at what their users are listening and finding relationships between users and songs.\n\nSpotify uses Implicit Matrix Factorization for recommendations, which relies on implicit data collected from users, rather than a explicit rating given to a score. Spotify uses a binary score for if the users listened to a song, with a weighting system for how many times it was streamed. They use Alternating Least Squares to alternates back and forth from solve least squares equations for users and then items. They first fix the song vector, then solve for users, then fix the user vector, and solve for the songs, and continue to do that until convergence of the vectors. One way to speed this up is to splitting up the work into blocks, each block only refers to a subset of users and songs. \n\nBecause our above Implicit Matrix Factorization algorithm is an iterative algorithm, during each phase, and each iteration it's performing another loop, so we're continually writing/reading to disk. Spotify started to use Spark to load the ratings matrix into memory, and avoid the issue of continually rereading it from disk. The final and most successful model they settled on was the Half-Gridify, caches the user/items partitions and includes all ratings for a user are in the same block. Because the ratings get cached and aren't shuffled around, once item vectors are joined with ratings partitions each partition has enough info to solve the optimal user vectors with no more additional shuffling (unlike the full gridify). The cost for this is that each partition could potentially require a copy of each item vector, which could potentially requires more local memory and may not actually fit in memory \n\nIt was interesting that at the time of the talk they hadn't actually been able to run the model on the full dataset without errors and timing out, suggesting there were still more improvements to be made. I was excited to see the process they went through and the explanations for why and how they adjusted each new attempt.\n",
      "metadata": {}
    }
  ]
}
